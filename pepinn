#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
PE-PINN: Projection Enhanced Physics-Informed Neural Networks
==============================================================

A PyTorch implementation of PE-PINN for solving radiative transfer 
critical eigenvalue problems.

Author: Wei Li, Yan Ma, Meng Zhu, Hong-e Ren, Shi-ran Geng
License: MIT
Version: 1.0.0
"""

import os
import json
import time
import logging
from typing import Dict, Tuple, Optional, Callable, Any
from dataclasses import dataclass, asdict
from pathlib import Path

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt
import pandas as pd
import yaml
from tqdm import tqdm

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# ============================================================================
# Configuration Classes
# ============================================================================

@dataclass
class TrainingConfig:
    """Training configuration parameters."""
    n_epochs: int = 5000
    batch_size: int = 512
    learning_rate_model: float = 1e-4
    learning_rate_sigma: float = 5e-4
    n_collocation: int = 1000
    n_boundary: int = 64
    gradient_clip: float = 1.0
    early_stopping_patience: int = 500
    checkpoint_freq: int = 100
    device: str = "cuda" if torch.cuda.is_available() else "cpu"
    seed: int = 42
    use_tensorboard: bool = True
    output_dir: str = "results"

@dataclass
class ModelConfig:
    """Neural network model configuration."""
    hidden_dim: int = 128
    n_layers: int = 5
    activation: str = "tanh"
    dropout: float = 0.03
    weight_init: str = "xavier_normal"
    use_batch_norm: bool = False

@dataclass
class SolverConfig:
    """PINN solver configuration."""
    n_gauss: int = 16
    domain: list = None
    boundary_weight: float = 10.0
    pde_weight: float = 1.0
    regularization_weight: float = 0.01
    
    def __post_init__(self):
        if self.domain is None:
            self.domain = [0.0, 1.0]

@dataclass
class ExperimentConfig:
    """Complete experiment configuration."""
    name: str
    description: str
    training: TrainingConfig
    model: ModelConfig
    solver: SolverConfig
    
    @classmethod
    def from_yaml(cls, path: str) -> 'ExperimentConfig':
        """Load configuration from YAML file."""
        with open(path, 'r') as f:
            config_dict = yaml.safe_load(f)
        
        return cls(
            name=config_dict['name'],
            description=config_dict['description'],
            training=TrainingConfig(**config_dict['training']),
            model=ModelConfig(**config_dict['model']),
            solver=SolverConfig(**config_dict['solver'])
        )
    
    def save(self, path: str):
        """Save configuration to YAML file."""
        config_dict = {
            'name': self.name,
            'description': self.description,
            'training': asdict(self.training),
            'model': asdict(self.model),
            'solver': asdict(self.solver)
        }
        
        with open(path, 'w') as f:
            yaml.dump(config_dict, f, default_flow_style=False)

# ============================================================================
# Base Classes
# ============================================================================

class BaseNN(nn.Module):
    """Base neural network class with common functionality."""
    
    def __init__(self, config: ModelConfig):
        super().__init__()
        self.config = config
        self.network = self._build_network()
        self._initialize_weights()
    
    def _build_network(self) -> nn.Sequential:
        """Build the neural network architecture."""
        raise NotImplementedError
    
    def _initialize_weights(self):
        """Initialize network weights."""
        for m in self.modules():
            if isinstance(m, nn.Linear):
                if self.config.weight_init == "xavier_normal":
                    nn.init.xavier_normal_(m.weight, gain=0.5)
                elif self.config.weight_init == "xavier_uniform":
                    nn.init.xavier_uniform_(m.weight, gain=0.5)
                elif self.config.weight_init == "he_normal":
                    nn.init.kaiming_normal_(m.weight, nonlinearity='relu')
                nn.init.zeros_(m.bias)
    
    def get_activation(self) -> nn.Module:
        """Get activation function based on config."""
        activations = {
            'relu': nn.ReLU(),
            'tanh': nn.Tanh(),
            'sigmoid': nn.Sigmoid(),
            'elu': nn.ELU(),
            'leaky_relu': nn.LeakyReLU(0.1),
            'gelu': nn.GELU()
        }
        return activations.get(self.config.activation, nn.Tanh())

class BasePINNSolver:
    """Base class for PINN solvers."""
    
    def __init__(self, config: SolverConfig):
        self.config = config
        self.device = torch.device(config.device if hasattr(config, 'device') else 'cpu')
        self._setup_quadrature()
    
    def _setup_quadrature(self):
        """Setup Gauss-Legendre quadrature."""
        mu_nodes, weights = np.polynomial.legendre.leggauss(self.config.n_gauss)
        self.mu_gauss = torch.tensor(mu_nodes, dtype=torch.float32).to(self.device)
        self.w_gauss = torch.tensor(weights, dtype=torch.float32).to(self.device)
    
    def compute_scattering_integral(self, model: nn.Module, x: torch.Tensor, 
                                   sigma: torch.Tensor) -> torch.Tensor:
        """Compute the scattering integral term."""
        raise NotImplementedError
    
    def compute_pde_residual(self, model: nn.Module, sigma: torch.Tensor,
                           x: torch.Tensor, mu: torch.Tensor) -> torch.Tensor:
        """Compute PDE residual."""
        raise NotImplementedError
    
    def compute_boundary_loss(self, model: nn.Module) -> torch.Tensor:
        """Compute boundary condition loss."""
        raise NotImplementedError

# ============================================================================
# Neural Network Models
# ============================================================================

class RadiativeTransferNN1D(BaseNN):
    """1D Radiative Transfer Neural Network with enhanced features."""
    
    def __init__(self, config: ModelConfig):
        super().__init__(config)
    
    def _build_network(self) -> nn.Sequential:
        """Build 1D network architecture."""
        layers = []
        
        # Input layer
        layers.append(nn.Linear(2, self.config.hidden_dim))
        if self.config.use_batch_norm:
            layers.append(nn.BatchNorm1d(self.config.hidden_dim))
        layers.append(self.get_activation())
        
        # Hidden layers
        for i in range(self.config.n_layers - 2):
            layers.append(nn.Linear(self.config.hidden_dim, self.config.hidden_dim))
            if self.config.use_batch_norm:
                layers.append(nn.BatchNorm1d(self.config.hidden_dim))
            layers.append(self.get_activation())
            if self.config.dropout > 0 and i % 2 == 1:
                layers.append(nn.Dropout(self.config.dropout))
        
        # Output layer
        layers.append(nn.Linear(self.config.hidden_dim, 1))
        
        return nn.Sequential(*layers)
    
    def forward(self, x: torch.Tensor, mu: torch.Tensor) -> torch.Tensor:
        """Forward pass.
        
        Parameters
        ----------
        x : torch.Tensor
            Spatial coordinates, shape (batch_size, 1)
        mu : torch.Tensor
            Angular coordinates, shape (batch_size, 1)
        
        Returns
        -------
        torch.Tensor
            Radiation intensity, shape (batch_size, 1)
        """
        inp = torch.cat([x, mu], dim=1)
        out = self.network(inp)
        # Ensure positive output using softplus
        psi = torch.nn.functional.softplus(out) + 1e-6
        return psi

class RadiativeTransferNN2D(BaseNN):
    """2D Radiative Transfer Neural Network with enhanced features."""
    
    def __init__(self, config: ModelConfig):
        # Adjust config for 2D
        config = ModelConfig(
            hidden_dim=config.hidden_dim * 2,  # Larger network for 2D
            n_layers=config.n_layers + 1,      # Deeper network
            activation=config.activation,
            dropout=config.dropout * 1.5,      # More regularization
            weight_init=config.weight_init,
            use_batch_norm=config.use_batch_norm
        )
        super().__init__(config)
    
    def _build_network(self) -> nn.Sequential:
        """Build 2D network architecture."""
        layers = []
        
        # Input layer (4 inputs: x, y, mu, eta)
        layers.append(nn.Linear(4, self.config.hidden_dim))
        if self.config.use_batch_norm:
            layers.append(nn.BatchNorm1d(self.config.hidden_dim))
        layers.append(self.get_activation())
        
        # Hidden layers
        for i in range(self.config.n_layers - 2):
            layers.append(nn.Linear(self.config.hidden_dim, self.config.hidden_dim))
            if self.config.use_batch_norm:
                layers.append(nn.BatchNorm1d(self.config.hidden_dim))
            layers.append(self.get_activation())
            if self.config.dropout > 0 and i % 2 == 1:
                layers.append(nn.Dropout(self.config.dropout))
        
        # Output layer
        layers.append(nn.Linear(self.config.hidden_dim, 1))
        
        return nn.Sequential(*layers)
    
    def forward(self, x: torch.Tensor, y: torch.Tensor, 
                mu: torch.Tensor, eta: torch.Tensor) -> torch.Tensor:
        """Forward pass for 2D model.
        
        Parameters
        ----------
        x, y : torch.Tensor
            Spatial coordinates, shape (batch_size, 1)
        mu, eta : torch.Tensor
            Angular coordinates, shape (batch_size, 1)
        
        Returns
        -------
        torch.Tensor
            Radiation intensity, shape (batch_size, 1)
        """
        inp = torch.cat([x, y, mu, eta], dim=1)
        out = self.network(inp)
        psi = torch.nn.functional.softplus(out) + 1e-6
        return psi

# ============================================================================
# PINN Solvers
# ============================================================================

class IncidentBoundaryPINN1D(BasePINNSolver):
    """1D Incident Boundary Condition PINN Solver.
    
    Solves the radiative transfer equation with incident boundary conditions
    where particles enter from both boundaries.
    """
    
    def compute_scattering_integral(self, model: nn.Module, x: torch.Tensor, 
                                   sigma: torch.Tensor) -> torch.Tensor:
        """Compute scattering integral using Gauss-Legendre quadrature."""
        batch_size = x.shape[0]
        integral = torch.zeros((batch_size, 1), device=x.device)
        
        for i in range(self.config.n_gauss):
            mu_i = self.mu_gauss[i].expand(batch_size, 1)
            w_i = self.w_gauss[i]
            psi_i = model(x, mu_i)
            integral += w_i * psi_i
        
        return (sigma / 2.0) * integral
    
    def compute_pde_residual(self, model: nn.Module, sigma: torch.Tensor,
                           x: torch.Tensor, mu: torch.Tensor) -> torch.Tensor:
        """Compute PDE residual for the radiative transfer equation."""
        x_grad = x.clone().requires_grad_(True)
        psi = model(x_grad, mu)
        
        # Compute gradient
        psi_x = torch.autograd.grad(
            outputs=psi, inputs=x_grad,
            grad_outputs=torch.ones_like(psi),
            create_graph=True, retain_graph=True
        )[0]
        
        # Compute scattering integral
        scatter = self.compute_scattering_integral(model, x_grad, sigma)
        
        # PDE residual: mu * dpsi/dx + sigma * psi - scatter = 0
        residual = mu * psi_x + sigma * psi - scatter
        return residual
    
    def compute_boundary_loss(self, model: nn.Module) -> torch.Tensor:
        """Compute incident boundary condition loss."""
        n_bc = self.config.n_boundary if hasattr(self.config, 'n_boundary') else 64
        
        # Left boundary: psi(0, mu>0) = 1
        x_left = torch.zeros((n_bc, 1), device=self.device)
        mu_left = torch.rand((n_bc, 1), device=self.device)  # mu > 0
        psi_left = model(x_left, mu_left)
        loss_left = torch.mean((psi_left - 1.0) ** 2)
        
        # Right boundary: psi(1, mu<0) = 0
        x_right = torch.ones((n_bc, 1), device=self.device)
        mu_right = -torch.rand((n_bc, 1), device=self.device)  # mu < 0
        psi_right = model(x_right, mu_right)
        loss_right = torch.mean(psi_right ** 2)
        
        return loss_left + loss_right

class ReflectiveBoundaryPINN1D(BasePINNSolver):
    """1D Reflective Boundary Condition PINN Solver.
    
    Solves the radiative transfer equation with reflective boundary conditions
    modeling partial reflection at boundaries.
    """
    
    def __init__(self, config: SolverConfig, alpha: float = 0.5, beta: float = 0.5):
        super().__init__(config)
        self.alpha = alpha  # Left boundary reflection coefficient
        self.beta = beta   # Right boundary reflection coefficient
    
    def compute_scattering_integral(self, model: nn.Module, x: torch.Tensor, 
                                   sigma: torch.Tensor) -> torch.Tensor:
        """Compute scattering integral using Gauss-Legendre quadrature."""
        batch_size = x.shape[0]
        integral = torch.zeros((batch_size, 1), device=x.device)
        
        for i in range(self.config.n_gauss):
            mu_i = self.mu_gauss[i].expand(batch_size, 1)
            w_i = self.w_gauss[i]
            psi_i = model(x, mu_i)
            integral += w_i * psi_i
        
        return (sigma / 2.0) * integral
    
    def compute_pde_residual(self, model: nn.Module, sigma: torch.Tensor,
                           x: torch.Tensor, mu: torch.Tensor) -> torch.Tensor:
        """Compute PDE residual for the radiative transfer equation."""
        x_grad = x.clone().requires_grad_(True)
        psi = model(x_grad, mu)
        
        psi_x = torch.autograd.grad(
            outputs=psi, inputs=x_grad,
            grad_outputs=torch.ones_like(psi),
            create_graph=True, retain_graph=True
        )[0]
        
        scatter = self.compute_scattering_integral(model, x_grad, sigma)
        residual = mu * psi_x + sigma * psi - scatter
        return residual
    
    def compute_boundary_loss(self, model: nn.Module) -> torch.Tensor:
        """Compute reflective boundary condition loss."""
        n_bc = self.config.n_boundary if hasattr(self.config, 'n_boundary') else 64
        
        # Left boundary: psi(0, mu>0) = alpha * psi(0, -mu)
        x_left = torch.zeros((n_bc, 1), device=self.device)
        mu_pos = torch.rand((n_bc, 1), device=self.device)
        mu_neg = -mu_pos
        
        psi_pos = model(x_left, mu_pos)
        psi_neg = model(x_left, mu_neg)
        loss_left = torch.mean((psi_pos - self.alpha * psi_neg) ** 2)
        
        # Right boundary: psi(1, mu<0) = beta * psi(1, -mu)
        x_right = torch.ones((n_bc, 1), device=self.device)
        mu_neg_r = -torch.rand((n_bc, 1), device=self.device)
        mu_pos_r = -mu_neg_r
        
        psi_neg_r = model(x_right, mu_neg_r)
        psi_pos_r = model(x_right, mu_pos_r)
        loss_right = torch.mean((psi_neg_r - self.beta * psi_pos_r) ** 2)
        
        return loss_left + loss_right

# ============================================================================
# Training and Evaluation
# ============================================================================

class PEPINNTrainer:
    """Enhanced trainer for PE-PINN models with advanced features."""
    
    def __init__(self, config: ExperimentConfig):
        self.config = config
        self.device = torch.device(config.training.device)
        
        # Setup directories
        self.output_dir = Path(config.training.output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # Setup logging
        self.setup_logging()
        
        # Initialize tracking
        self.history = {
            'epoch': [], 'sigma': [], 'error': [],
            'total_loss': [], 'pde_loss': [], 'bc_loss': [],
            'learning_rate': [], 'time_elapsed': []
        }
        
        # Best model tracking
        self.best_loss = float('inf')
        self.best_epoch = 0
        self.patience_counter = 0
    
    def setup_logging(self):
        """Setup file and console logging."""
        log_file = self.output_dir / f"{self.config.name}_training.log"
        file_handler = logging.FileHandler(log_file)
        file_handler.setLevel(logging.INFO)
        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
        file_handler.setFormatter(formatter)
        logger.addHandler(file_handler)
    
    def train(self, model: nn.Module, solver: BasePINNSolver) -> Tuple[Dict, float]:
        """Train the PE-PINN model.
        
        Parameters
        ----------
        model : nn.Module
            Neural network model
        solver : BasePINNSolver
            PINN solver instance
        
        Returns
        -------
        Tuple[Dict, float]
            Training history and final sigma value
        """
        logger.info(f"Starting training: {self.config.name}")
        logger.info(f"Configuration: {self.config}")
        
        # Move model to device
        model = model.to(self.device)
        
        # Initialize parameters
        sigma = torch.tensor([1.0], requires_grad=True, device=self.device)
        
        # Setup optimizers
        optimizer_model = optim.Adam(model.parameters(), lr=self.config.training.learning_rate_model)
        optimizer_sigma = optim.Adam([sigma], lr=self.config.training.learning_rate_sigma)
        
        # Learning rate scheduler
        scheduler_model = optim.lr_scheduler.CosineAnnealingLR(
            optimizer_model, T_max=self.config.training.n_epochs
        )
        scheduler_sigma = optim.lr_scheduler.CosineAnnealingLR(
            optimizer_sigma, T_max=self.config.training.n_epochs
        )
        
        # Training loop
        start_time = time.time()
        pbar = tqdm(range(self.config.training.n_epochs), desc="Training")
        
        for epoch in pbar:
            # Sample training data
            x_pde, mu_pde = self.sample_collocation_points()
            
            # Forward pass
            residual = solver.compute_pde_residual(model, sigma, x_pde, mu_pde)
            loss_pde = torch.mean(residual ** 2)
            
            loss_bc = solver.compute_boundary_loss(model)
            
            # Total loss with weights
            total_loss = (self.config.solver.pde_weight * loss_pde + 
                         self.config.solver.boundary_weight * loss_bc)
            
            # Add regularization if specified
            if self.config.solver.regularization_weight > 0:
                l2_reg = sum(p.pow(2.0).sum() for p in model.parameters())
                total_loss += self.config.solver.regularization_weight * l2_reg
            
            # Backward pass
            optimizer_model.zero_grad()
            optimizer_sigma.zero_grad()
            total_loss.backward()
            
            # Gradient clipping
            if self.config.training.gradient_clip > 0:
                torch.nn.utils.clip_grad_norm_(model.parameters(), 
                                              self.config.training.gradient_clip)
            
            optimizer_model.step()
            optimizer_sigma.step()
            
            # Learning rate scheduling
            scheduler_model.step()
            scheduler_sigma.step()
            
            # Calculate metrics
            with torch.no_grad():
                sigma_value = sigma.item()
                error = abs(sigma_value - 1.0)  # Assuming true value is 1.0
                
                # Update history
                if epoch % 10 == 0:
                    self.history['epoch'].append(epoch)
                    self.history['sigma'].append(sigma_value)
                    self.history['error'].append(error)
                    self.history['total_loss'].append(total_loss.item())
                    self.history['pde_loss'].append(loss_pde.item())
                    self.history['bc_loss'].append(loss_bc.item())
                    self.history['learning_rate'].append(optimizer_model.param_groups[0]['lr'])
                    self.history['time_elapsed'].append(time.time() - start_time)
                
                # Update progress bar
                pbar.set_postfix({
                    'σ': f'{sigma_value:.4f}',
                    'error': f'{error:.4f}',
                    'loss': f'{total_loss.item():.2e}'
                })
                
                # Early stopping check
                if total_loss.item() < self.best_loss:
                    self.best_loss = total_loss.item()
                    self.best_epoch = epoch
                    self.patience_counter = 0
                    
                    # Save best model
                    self.save_checkpoint(model, sigma, epoch, 'best')
                else:
                    self.patience_counter += 1
                    if self.patience_counter >= self.config.training.early_stopping_patience:
                        logger.info(f"Early stopping at epoch {epoch}")
                        break
                
                # Regular checkpointing
                if epoch % self.config.training.checkpoint_freq == 0:
                    self.save_checkpoint(model, sigma, epoch)
        
        # Training completed
        total_time = time.time() - start_time
        self.history['total_training_time'] = total_time
        self.history['best_epoch'] = self.best_epoch
        
        logger.info(f"Training completed in {total_time:.2f} seconds")
        logger.info(f"Best epoch: {self.best_epoch}, Best loss: {self.best_loss:.6f}")
        logger.info(f"Final σ: {sigma_value:.6f}, Error: {error:.6f}")
        
        # Save final results
        self.save_results()
        
        return self.history, sigma_value
    
    def sample_collocation_points(self) -> Tuple[torch.Tensor, torch.Tensor]:
        """Sample collocation points for PDE residual."""
        n_points = self.config.training.n_collocation
        
        # Latin hypercube sampling for better coverage
        x = self.latin_hypercube_sample(n_points, 1)
        mu = 2 * self.latin_hypercube_sample(n_points, 1) - 1  # Scale to [-1, 1]
        
        x = torch.tensor(x, dtype=torch.float32, device=self.device)
        mu = torch.tensor(mu, dtype=torch.float32, device=self.device)
        
        return x, mu
    
    def latin_hypercube_sample(self, n_samples: int, n_dims: int) -> np.ndarray:
        """Generate Latin hypercube samples."""
        samples = np.zeros((n_samples, n_dims))
        
        for i in range(n_dims):
            samples[:, i] = (np.random.permutation(n_samples) + 
                           np.random.uniform(size=n_samples)) / n_samples
        
        return samples
    
    def save_checkpoint(self, model: nn.Module, sigma: torch.Tensor, 
                       epoch: int, tag: str = None):
        """Save model checkpoint."""
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'sigma': sigma.item(),
            'config': self.config
        }
        
        if tag:
            filename = f"checkpoint_{tag}.pt"
        else:
            filename = f"checkpoint_epoch_{epoch}.pt"
        
        torch.save(checkpoint, self.output_dir / filename)
    
    def save_results(self):
        """Save training results."""
        # Save history as CSV
        df = pd.DataFrame(self.history)
        df.to_csv(self.output_dir / 'training_history.csv', index=False)
        
        # Save configuration
        self.config.save(self.output_dir / 'config.yaml')
        
        # Create summary JSON
        summary = {
            'experiment_name': self.config.name,
            'final_sigma': self.history['sigma'][-1] if self.history['sigma'] else None,
            'final_error': self.history['error'][-1] if self.history['error'] else None,
            'best_epoch': self.best_epoch,
            'best_loss': self.best_loss,
            'total_training_time': self.history.get('total_training_time', 0),
            'n_parameters': sum(p.numel() for p in model.parameters() 
                              if hasattr(self, 'model'))
        }
        
        with open(self.output_dir / 'summary.json', 'w') as f:
            json.dump(summary, f, indent=2)

# ============================================================================
# Visualization
# ============================================================================

class Visualizer:
    """Visualization utilities for PE-PINN results."""
    
    @staticmethod
    def plot_training_history(history: Dict, save_path: Optional[str] = None):
        """Plot training history with multiple metrics."""
        fig, axes = plt.subplots(2, 3, figsize=(15, 8))
        
        # Parameter convergence
        axes[0, 0].plot(history['epoch'], history['sigma'], 'b-', linewidth=2)
        axes[0, 0].axhline(y=1.0, color='r', linestyle='--', label='True value')
        axes[0, 0].set_xlabel('Epoch')
        axes[0, 0].set_ylabel('σ')
        axes[0, 0].set_title('Parameter Convergence')
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)
        
        # Error evolution
        axes[0, 1].semilogy(history['epoch'], history['error'], 'g-', linewidth=2)
        axes[0, 1].set_xlabel('Epoch')
        axes[0, 1].set_ylabel('Relative Error')
        axes[0, 1].set_title('Error Evolution')
        axes[0, 1].grid(True, alpha=0.3)
        
        # Loss evolution
        axes[0, 2].semilogy(history['epoch'], history['total_loss'], 'r-', linewidth=2)
        axes[0, 2].set_xlabel('Epoch')
        axes[0, 2].set_ylabel('Total Loss')
        axes[0, 2].set_title('Loss Evolution')
        axes[0, 2].grid(True, alpha=0.3)
        
        # PDE and BC losses
        axes[1, 0].semilogy(history['epoch'], history['pde_loss'], 'b-', 
                           label='PDE', linewidth=2)
        axes[1, 0].semilogy(history['epoch'], history['bc_loss'], 'r-', 
                           label='BC', linewidth=2)
        axes[1, 0].set_xlabel('Epoch')
        axes[1, 0].set_ylabel('Loss')
        axes[1, 0].set_title('Component Losses')
        axes[1, 0].legend()
        axes[1, 0].grid(True, alpha=0.3)
        
        # Learning rate
        axes[1, 1].semilogy(history['epoch'], history['learning_rate'], 'k-', linewidth=2)
        axes[1, 1].set_xlabel('Epoch')
        axes[1, 1].set_ylabel('Learning Rate')
        axes[1, 1].set_title('Learning Rate Schedule')
        axes[1, 1].grid(True, alpha=0.3)
        
        # Training efficiency
        axes[1, 2].plot(history['time_elapsed'], history['error'], 'm-', linewidth=2)
        axes[1, 2].set_xlabel('Time (s)')
        axes[1, 2].set_ylabel('Error')
        axes[1, 2].set_title('Training Efficiency')
        axes[1, 2].grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
        else:
            plt.show()
        
        plt.close()

# ============================================================================
# Main Execution
# ============================================================================

def main():
    """Main execution function."""
    # Create default configuration
    config = ExperimentConfig(
        name="PE-PINN_Incident_Boundary",
        description="1D Radiative Transfer with Incident Boundary Conditions",
        training=TrainingConfig(n_epochs=5000),
        model=ModelConfig(hidden_dim=128, n_layers=5),
        solver=SolverConfig(n_gauss=16)
    )
    
    # Initialize model and solver
    model = RadiativeTransferNN1D(config.model)
    solver = IncidentBoundaryPINN1D(config.solver)
    
    # Create trainer and train
    trainer = PEPINNTrainer(config)
    history, final_sigma = trainer.train(model, solver)
    
    # Visualize results
    viz = Visualizer()
    viz.plot_training_history(history, save_path=config.training.output_dir + '/training_history.png')
    
    print(f"\nTraining completed!")
    print(f"Final σ: {final_sigma:.6f}")
    print(f"Relative error: {abs(final_sigma - 1.0):.6f}")
    print(f"Results saved to: {config.training.output_dir}")

if __name__ == "__main__":
    main()
